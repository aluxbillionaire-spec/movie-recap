{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0806e39d",
   "metadata": {},
   "source": [
    "# Movie Recap Pipeline - Real-ESRGAN 4K Upscaling\n",
    "\n",
    "GPU-accelerated 4K upscaling with chunked processing and Drive integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14584562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch torchvision opencv-python-headless Pillow requests tqdm\n",
    "!git clone https://github.com/xinntao/Real-ESRGAN.git\n",
    "%cd Real-ESRGAN\n",
    "!pip install -q basicsr facexlib gfpgan .\n",
    "!wget -q https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth -P weights/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72ae546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Drive and setup paths\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os, json, time, shutil, tempfile, subprocess, requests\n",
    "from pathlib import Path\n",
    "import torch, cv2, numpy as np\n",
    "from tqdm import tqdm\n",
    "from realesrgan import RealESRGANer\n",
    "from basicsr.archs.rrdbnet_arch import RRDBNet\n",
    "\n",
    "DRIVE_ROOT = '/content/drive/MyDrive/movie-recap-pipeline'\n",
    "UPSCALE_INPUTS = f'{DRIVE_ROOT}/upscale_inputs'\n",
    "UPSCALE_OUTPUTS = f'{DRIVE_ROOT}/upscale_outputs'\n",
    "CHECKPOINTS_DIR = f'{DRIVE_ROOT}/checkpoints'\n",
    "\n",
    "for path in [UPSCALE_INPUTS, UPSCALE_OUTPUTS, CHECKPOINTS_DIR]:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e6e1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoUpscaler:\n",
    "    def __init__(self, model_path='weights/RealESRGAN_x4plus.pth', scale=4):\n",
    "        self.scale = scale\n",
    "        self.model_path = model_path\n",
    "        self.upsampler = None\n",
    "        \n",
    "    def initialize_model(self):\n",
    "        model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=self.scale)\n",
    "        self.upsampler = RealESRGANer(\n",
    "            scale=self.scale, model_path=self.model_path, model=model,\n",
    "            tile=400, tile_pad=10, pre_pad=0, half=True,\n",
    "            gpu_id=0 if torch.cuda.is_available() else None\n",
    "        )\n",
    "        print(\"Model loaded\")\n",
    "        \n",
    "    def extract_frames(self, video_path, start_frame=0, chunk_size=50):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "        \n",
    "        frames = []\n",
    "        for _ in range(chunk_size):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret: break\n",
    "            frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        cap.release()\n",
    "        return frames, total_frames, fps\n",
    "        \n",
    "    def upscale_frames(self, frames):\n",
    "        upscaled = []\n",
    "        for i, frame in enumerate(tqdm(frames, desc=\"Upscaling\")):\n",
    "            try:\n",
    "                result, _ = self.upsampler.enhance(frame, outscale=self.scale)\n",
    "                upscaled.append(result)\n",
    "                if i % 10 == 0: torch.cuda.empty_cache()\n",
    "            except Exception as e:\n",
    "                print(f\"Frame {i} error: {e}\")\n",
    "                upscaled.append(frame)\n",
    "        return upscaled\n",
    "        \n",
    "    def process_video_chunked(self, input_path, output_path, job_id, webhook_url=None):\n",
    "        if not self.upsampler: self.initialize_model()\n",
    "        \n",
    "        checkpoint_path = f\"{CHECKPOINTS_DIR}/checkpoint_{job_id}.json\"\n",
    "        start_frame = 0\n",
    "        processed_chunks = []\n",
    "        \n",
    "        if os.path.exists(checkpoint_path):\n",
    "            with open(checkpoint_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                start_frame = data.get('last_frame', 0)\n",
    "                processed_chunks = data.get('processed_chunks', [])\n",
    "        \n",
    "        temp_dir = tempfile.mkdtemp()\n",
    "        \n",
    "        try:\n",
    "            cap = cv2.VideoCapture(input_path)\n",
    "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "            cap.release()\n",
    "            \n",
    "            chunk_num = len(processed_chunks)\n",
    "            current_frame = start_frame\n",
    "            \n",
    "            while current_frame < total_frames:\n",
    "                frames, _, _ = self.extract_frames(input_path, current_frame, 50)\n",
    "                if not frames: break\n",
    "                \n",
    "                upscaled = self.upscale_frames(frames)\n",
    "                \n",
    "                # Save chunk\n",
    "                chunk_path = f\"{temp_dir}/chunk_{chunk_num:04d}.mp4\"\n",
    "                height, width = upscaled[0].shape[:2]\n",
    "                out = cv2.VideoWriter(chunk_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "                for frame in upscaled:\n",
    "                    out.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "                out.release()\n",
    "                \n",
    "                processed_chunks.append(chunk_path)\n",
    "                current_frame += len(frames)\n",
    "                \n",
    "                # Save checkpoint\n",
    "                with open(checkpoint_path, 'w') as f:\n",
    "                    json.dump({\n",
    "                        'job_id': job_id, 'last_frame': current_frame,\n",
    "                        'processed_chunks': processed_chunks, 'total_frames': total_frames,\n",
    "                        'fps': fps, 'timestamp': time.time()\n",
    "                    }, f)\n",
    "                \n",
    "                progress = (current_frame / total_frames) * 100\n",
    "                print(f\"Progress: {progress:.1f}%\")\n",
    "                \n",
    "                if webhook_url:\n",
    "                    try:\n",
    "                        requests.post(webhook_url, json={'job_id': job_id, 'progress': progress}, timeout=10)\n",
    "                    except: pass\n",
    "                \n",
    "                chunk_num += 1\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            # Concatenate chunks\n",
    "            filelist = f\"{temp_dir}/filelist.txt\"\n",
    "            with open(filelist, 'w') as f:\n",
    "                for chunk in processed_chunks:\n",
    "                    f.write(f\"file '{chunk}'\\n\")\n",
    "            \n",
    "            cmd = ['ffmpeg', '-f', 'concat', '-safe', '0', '-i', filelist, '-c', 'copy', '-y', output_path]\n",
    "            subprocess.run(cmd, capture_output=True)\n",
    "            \n",
    "            if os.path.exists(checkpoint_path): os.remove(checkpoint_path)\n",
    "            \n",
    "            return {'status': 'success', 'output_path': output_path, 'total_frames': total_frames}\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {'status': 'error', 'error': str(e)}\n",
    "        finally:\n",
    "            shutil.rmtree(temp_dir, ignore_errors=True)\n",
    "\n",
    "upscaler = VideoUpscaler()\n",
    "print(\"VideoUpscaler ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aa6025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pending_jobs():\n",
    "    job_files = [f for f in os.listdir(UPSCALE_INPUTS) if f.endswith('.json')]\n",
    "    if not job_files:\n",
    "        print(\"No jobs found\")\n",
    "        return\n",
    "    \n",
    "    for job_file in job_files:\n",
    "        job_path = f\"{UPSCALE_INPUTS}/{job_file}\"\n",
    "        try:\n",
    "            with open(job_path, 'r') as f:\n",
    "                job = json.load(f)\n",
    "            \n",
    "            job_id = job['job_id']\n",
    "            video_path = job['video_path']\n",
    "            webhook_url = job.get('webhook_url')\n",
    "            \n",
    "            print(f\"Processing {job_id}\")\n",
    "            \n",
    "            output_dir = f\"{UPSCALE_OUTPUTS}/{job_id}\"\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            output_path = f\"{output_dir}/upscaled_video.mp4\"\n",
    "            \n",
    "            result = upscaler.process_video_chunked(video_path, output_path, job_id, webhook_url)\n",
    "            \n",
    "            with open(f\"{output_dir}/result.json\", 'w') as f:\n",
    "                json.dump(result, f, indent=2)\n",
    "            \n",
    "            if webhook_url and result['status'] == 'success':\n",
    "                completion_data = {\n",
    "                    'job_id': job_id, 'status': 'completed',\n",
    "                    'upscaled_video_path': output_path, 'timestamp': time.time()\n",
    "                }\n",
    "                try:\n",
    "                    requests.post(webhook_url, json=completion_data, timeout=30)\n",
    "                    print(f\"Notified completion for {job_id}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Notification failed: {e}\")\n",
    "            \n",
    "            # Move to completed\n",
    "            completed_dir = f\"{DRIVE_ROOT}/completed_jobs\"\n",
    "            os.makedirs(completed_dir, exist_ok=True)\n",
    "            shutil.move(job_path, f\"{completed_dir}/{job_file}\")\n",
    "            print(f\"Job {job_id} completed\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {job_file}: {e}\")\n",
    "            failed_dir = f\"{DRIVE_ROOT}/failed_jobs\"\n",
    "            os.makedirs(failed_dir, exist_ok=True)\n",
    "            shutil.move(job_path, f\"{failed_dir}/{job_file}\")\n",
    "\n",
    "def monitor_jobs(check_interval=60, max_runtime=3600):\n",
    "    print(f\"Monitoring started (check every {check_interval}s, max {max_runtime}s)\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while time.time() - start_time < max_runtime:\n",
    "        try:\n",
    "            process_pending_jobs()\n",
    "            time.sleep(check_interval)\n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Monitor error: {e}\")\n",
    "            time.sleep(check_interval)\n",
    "\n",
    "print(\"Ready to monitor jobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2db65ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start monitoring\n",
    "monitor_jobs(check_interval=60, max_runtime=3600)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
